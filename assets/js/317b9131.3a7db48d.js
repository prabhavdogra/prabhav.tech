"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8206],{9775:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"sync_once","metadata":{"permalink":"/prabhav.tech/blog/sync_once","editUrl":"https://github.com/prabhavdogra/prabhav.tech/tree/master/blog/2025-04-19-sync_once_internals/index.md","source":"@site/blog/2025-04-19-sync_once_internals/index.md","title":"Digging into sync.Once: How Go Ensures One-Time Execution","description":"How it started?","date":"2025-04-19T00:00:00.000Z","tags":[{"inline":false,"label":"Go","permalink":"/prabhav.tech/blog/tags/go","description":"Golang exploration projects"}],"readingTime":6.22,"hasTruncateMarker":false,"authors":[{"name":"Prabhav Dogra","title":"Software Engineer II @ Blinkit","url":"https://github.com/prabhavdogra","page":{"permalink":"/prabhav.tech/blog/authors/prabhavdogra"},"socials":{"github":"https://github.com/prabhavdogra","linkedin":"https://www.linkedin.com/in/prabhav-dogra/"},"imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGHJzJ1YVb_og/profile-displayphoto-shrink_800_800/B56ZS6fOAFHEAc-/0/1738295534859?e=1746057600&v=beta&t=wTqgffLNqUVaz0xEk2OUDSEvKATaSevxvWuI99mG9XY","key":"prabhavdogra"}],"frontMatter":{"slug":"sync_once","title":"Digging into sync.Once: How Go Ensures One-Time Execution","authors":["prabhavdogra"],"tags":["go"]},"unlisted":false,"nextItem":{"title":"What does memory mean actually?","permalink":"/prabhav.tech/blog/memory_hierarchy"}},"content":"### How it started?\\nWhile writing some concurrent code for [Blinkit](https://blinkit.com/), I found myself reaching for `sync.Once`\u2014a common utility in Go to ensure an action is performed just once, no matter how many goroutines attempt it. Out of curiosity, I decided to dig into how `sync.Once` works internally and how its implementation has evolved over time.\\n\\nIn this blog, I\u2019ll walk through the internals of `sync.Once`, how it leverages atomics for performance, and trace its evolution through Go versions. This blog is meant to motivate you to explore and solve your own doubts by diving into the source code of Go itself. You\u2019ll be amazed at how much you can learn just by following the code and seeing how things work behind the scenes!\\n\\n### Prerequisites: What\'s sync.Once?\\n\\n- `sync.Once` ensures a function is only executed once, no matter how many times it\'s called, even across goroutines.\\n- It\'s most commonly used to initialize shared resources like config, DB connections, or singletons.\\n\\n```go \\nvar (\\n\\treadConfigOnce  sync.Once\\n\\tconfig          *Config\\n)\\n\\nfunc GetConfig() (c *Config) {\\n\\treadConfigOnce.Do(func() {\\n\\t\\t// Read yaml and make config object\\n\\t})\\n\\treturn config\\n}\\n\\nfunc main() {\\n\\tcfg := LoadConfig()\\n\\tfmt.Println(cfg)\\n}\\n```\\nLink to [sync.Once documentation](https://pkg.go.dev/sync#Once.Do)\\n\\n## Digging into sync.Once internals?\\nTo follow along this read feel free to clone the [Golang](https://github.com/golang/go) repository.\\n- Open the repository and run the bash script `./make.bash`, to build and install the latest compiler of Go.\\n- Point your system or editor (like VSCode) to use the newly built Go version:\\n\\n:::note Bootstrapped Compilers\\nWhen I first came across the concept of a bootstrapped compiler, it honestly felt like a total brain teaser. The idea that a compiler could be written in the same language it\u2019s supposed to compile? Wild.\\n\\nHere\u2019s the bombshell: **the Go compiler is written in Go itself**.\\nSounds paradoxical, right?\\n\\nLike a classic chicken-and-egg dilemma - \\n**\u201cHow can a compiler compile itself if it doesn\u2019t exist yet?\u201d**\\n> In programming, bootstrapping refers to:\\n> The process of building a system using a simpler or initial version of itself.\\n\\n- `make.bash` is a shell script located at `src/make.bash` inside the Go source tree.\\n- It\'s used to bootstrap the Go toolchain \u2014 it builds the Go compiler (`cmd/compile`), linker (`cmd/link`), and other core tools from scratch using the Go bootstrap toolchain.\\n- It uses the already installed Go compiler\\n- Use the clones Golang source code to build the new version of Go.\\n:::\\n\\n### Internals (Go 1.18)\\nLet\'s start from the basics, struct of `sync.Once` looks like\\n```go title=\\"src/sync/once.go | GOVERSION=1.18\\"\\ntype Once struct {\\n\\tdone uint32\\n\\tm    Mutex\\n}\\n```\\nPretty simple, right?\\n- One `uint32` flag `done`\\n    - `done == 1` means that the function has ran once\\n    - `done == 0` means that the function hasn\'t ran yet\\n- One mutex `m`\\n    - A mutex to avoid race condition while updating `done`\\n\\nOk, somehow these two are used together to:\\n- Ensure that a particular action executes only once, regardless of how many times it is called concurrently.\\n- Achieve this guarantee efficiently, minimizing lock contention for better performance.\\n\\nOk so far so good right? \\n\\nLet\'s move to the implementation of `once.Do(f)`: It ensures that the function `f()` is only executed once, no matter how many times it\'s called\u2014even if from multiple goroutines.\\n\\n```go title=\\"src/sync/once.go | GOVERSION=1.18\\"\\nfunc (o *Once) Do(f func()) {\\n\\tif atomic.LoadUint32(&o.done) == 0 {\\n\\t\\to.doSlow(f)\\n\\t}\\n}\\n```\\n\\n**Goal:** Avoid acquiring a mutex unless absolutely necessary (i.e., the function `f()` hasn\u2019t run yet).\\n- `if atomic.LoadUint32(&o.done) == 0`\\n    - We check if `done == 0`, `done == 0` means that the function hasn\'t ran yet\\n    - It checks `done == 0` atomically in one operation. More about [`atomic package`](https://prabhavdogra.github.io/prabhav.tech/blog/atomic_operations)\\n\\n```go\\nfunc (o *Once) doSlow(f func()) {\\n\\to.m.Lock()\\n\\tdefer o.m.Unlock()\\n\\tif o.done == 0 {\\n\\t\\tdefer atomic.StoreUint32(&o.done, 1)\\n\\t\\tf()\\n\\t}\\n}\\n```\\n\\n* `if o.done == 0`\\n    * A second check inside the locked section.\\n    * Why? Because multiple goroutines might pass the atomic check in Do(), but only one should actually run the function. So we check again inside the lock to be 100% sure.\\n    * This is a double-checked locking pattern.\\n\\n- `defer atomic.StoreUint32(&o.done, 1)`\\n    - Marks the function as executed after `f()` is done.\\n    - It\u2019s deferred so even if `f()` panics, we still consider it \u201cdone\u201d and don\'t call it again (intentional in Go\u2019s design).\\n\\n### Internals (Go 1.24)\\nIn newer versions of Go they revised the implementation of how `sync.Once`.\\n```go title=\\"src/sync/once.go | GOVERSION=1.24\\"\\ntype Once struct {\\n\\t_ noCopy\\n\\tdone atomic.Uint32\\n\\tm    Mutex\\n}\\n```\\nOk!\\nWhat\'s changed now?\\n- `noCopy` is embedded in `Once` struct\\n- done is `atomic.Uint32` not `uint32`\\n\\n#### \\"noCopy\\" What\'s that?\\n- It\'a a zero-size struct that is adds no memory overhead.\\n- Go has a statical analysis tool `go vet` that checks your Go source code for common mistakes and suspicious constructs that the compiler won\u2019t catch.\\n- Some types must never be copied once they\u2019ve been initialized\u2014most notably synchronization primitives like sync.Mutex, sync.Once, etc. Accidental copies can lead to deadlocks or data races.\\n- Having `noCopy` embedded in your struct will produce a warning if your type is ever copied by `go vet`.\\n\\n#### Regular uint32 vs. atomic.Uint32\\nWhen you don\'t know something in Go let\'s follow the approach like we have done, and let\'s look at the source code:\\n```go title=\\"src/atomic/type.go | GOVERSION=1.24\\"\\ntype Uint32 struct {\\n\\t_ noCopy\\n\\tv uint32\\n}\\n```\\n\\n- Ok, wow! As you can see `atomic.Uint32` is just a wrapper type around a uint32 with `noCopy` but why???\\n\\nLet\'s look further functions binded to this struct:\\n```go\\n// Load atomically loads and returns the value stored in x.\\nfunc (x *Uint32) Load() uint32 { return LoadUint32(&x.v) }\\n\\n// Store atomically stores val into x.\\nfunc (x *Uint32) Store(val uint32) { StoreUint32(&x.v, val) }\\n\\n// Swap atomically stores new into x and returns the previous value.\\nfunc (x *Uint32) Swap(new uint32) (old uint32) { return SwapUint32(&x.v, new) }\\n\\n// CompareAndSwap executes the compare-and-swap operation for x.\\nfunc (x *Uint32) CompareAndSwap(old, new uint32) (swapped bool) { return CompareAndSwapUint32(&x.v, old, new) }\\n```\\nOk, seems like it\'s just a wrapper type provides methods for atomic operations.\\n\\nAnd that is exactly what `atomic.Uint32` is:\\n> A Go\xa01.19+ wrapper type around a uint32 that provides methods for atomic operations\\n\\n## Conclusion:\\n\\nExploring `sync.Once` from **Go\xa01.18** to **Go\xa01.24** shows how a small, fundamental primitive can evolve for clarity, safety, and maintainability:\\n\\n- **Go\xa01.18**\\n    - Used a plain uint32 flag plus a Mutex and double\u2011checked locking\\n    - Minimized lock contention by atomically checking the flag on the fast path\\n\\n* **Go\xa01.24**\\n    * Embeds noCopy to catch accidental copies via go vet\\n    * Switches to atomic.Uint32, providing a clean, method\u2011based API\\nAlong the way we\u2019ve seen:\\n- **Bootstrapping** \u2013 how Go builds itself from source via make.bash\\n- **Atomic vs. mutex** \u2013 why lock\u2011free fast paths matter in high\u2011concurrency code\\n- **Static analysis** \u2013 how noCopy and go vet help prevent subtle bugs\\n\\nThe beauty of Go\u2019s standard library is that it balances performance, safety, and readability. Whenever you have a question about how Go works under the hood, the answer is just a GitHub clone and a make.bash away. Dive into the source, follow the code, and you\u2019ll not only solve your doubts\u2014you\u2019ll discover deeper principles that make Go such a pleasure to work with.\\nAnd it\u2019s downright fun to see how these technologies evolve over time.\\n\\n[Go 1.19 Release Notes](https://tip.golang.org/doc/go1.19)"},{"id":"memory_hierarchy","metadata":{"permalink":"/prabhav.tech/blog/memory_hierarchy","editUrl":"https://github.com/prabhavdogra/prabhav.tech/tree/master/blog/2025-03-12-storage-types/index.md","source":"@site/blog/2025-03-12-storage-types/index.md","title":"What does memory mean actually?","description":"Introduction","date":"2025-03-12T00:00:00.000Z","tags":[{"inline":false,"label":"Go","permalink":"/prabhav.tech/blog/tags/go","description":"Golang exploration projects"}],"readingTime":11.48,"hasTruncateMarker":false,"authors":[{"name":"Prabhav Dogra","title":"Software Engineer II @ Blinkit","url":"https://github.com/prabhavdogra","page":{"permalink":"/prabhav.tech/blog/authors/prabhavdogra"},"socials":{"github":"https://github.com/prabhavdogra","linkedin":"https://www.linkedin.com/in/prabhav-dogra/"},"imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGHJzJ1YVb_og/profile-displayphoto-shrink_800_800/B56ZS6fOAFHEAc-/0/1738295534859?e=1746057600&v=beta&t=wTqgffLNqUVaz0xEk2OUDSEvKATaSevxvWuI99mG9XY","key":"prabhavdogra"}],"frontMatter":{"slug":"memory_hierarchy","title":"What does memory mean actually?","authors":["prabhavdogra"],"tags":["go"]},"unlisted":false,"prevItem":{"title":"Digging into sync.Once: How Go Ensures One-Time Execution","permalink":"/prabhav.tech/blog/sync_once"},"nextItem":{"title":"Writing your own Goroutines","permalink":"/prabhav.tech/blog/diy_goroutines"}},"content":"### Introduction\\n\\nWhile exploring how Go manages memory, I stumbled upon an intricate hierarchy that determines how fast data moves between the CPU and RAM. Go\u2019s runtime optimizations, like garbage collection and stack allocation, made me curious about what happens under the hood. This led me to registers, caches (L1, L2, L3), and RAM\u2014each playing a crucial role in balancing speed and storage.\\n\\n## Prerequisites: What\'s a CPU Cycle?\\n**CPU Cycle:** \\n    - It\'s the smallest unit of processing that a CPU can do.\\n    - Each cycle allows the CPU to execute instructions like fetching data, performing arithmetic, or storing results.\\n    - For example: the method `atomic.CompareAndSwap` in go is executed as follows:\\n        - It reads a value from memory.\\n        - It compares it with an expected value.\\n        - If they match, it writes a new value.\\n        - This requires at least three steps (read, compare, write), which take multiple cycles.\\n    - A single 1GHz CPU can complete one CPU cycle in 1 nanosecond.\\n    - Similarly, a 3GHz CPU can complete one CPU cycle in 0.33 nanoseconds.\\n    - Not every operation takes 1 cycle\\n        - Simple instructions like integer addition (a + b) may take 1 cycle.\\n        - More complex operations (e.g., division, memory access) can take multiple cycles.\\n\\n![Clock edge](image.png)\\n\\n**NOTE:** Here **Period = Clock Cycle**\\n\\n**Clock edge**\\nA clock edge refers to the transition point of a clock signal where changes in a digital circuit occur. The clock signal is a periodic waveform (square wave), and it has two main edges:\\n- **Rising Edge (Positive Edge)**\\n    - The transition from low (0) to high (1).\\n    - Many digital circuits, including registers and flip-flops, are designed to capture input and update their state on this edge.\\n- **Falling Edge (Negative Edge)**\\n    - The transition from high (1) to low (0).\\n    - Some circuits use this edge for synchronization, though it is less common than the rising edge.\\n\\n**Why is the Clock Edge Important?**\\n- It synchronizes operations in digital circuits.\\n- Registers and flip-flops capture and store data only on a specific edge, ensuring controlled data flow.\\n- In CPU pipelines, clock edges trigger different stages like instruction fetch, decode, execute, etc.\\n\\n\\n## Understanding Computer Memory Hierarchy\\n1. **Registers (Fastest, Smallest)**\\n    - Located inside the CPU, closest to the execution units.\\n    - Stores data for immediate operations (e.g., arithmetic calculations).\\n    - Extremely small (few bytes) but operates at CPU clock speed.\\n    - **Access time:** 1 CPU cycle (fastest).\\n2. **L1 Cache (Level 1)**\\n    * Smallest and fastest cache (typically 32KB to 128KB per core).\\n    * Directly integrated into the CPU core.\\n    * Stores frequently used instructions (will discuss this later) and data for ultrafast access.\\n    * **Access time:** 2-4 CPU cycles.\\n3. **L2 Cache (Level 2)**\\n    - Larger than L1 (256KB to a few MB per core).\\n    - Slightly slower than L1 but still much faster than RAM.\\n    - Used to store recently accessed data that might be needed again soon.\\n    - **Access time:** 10-20 CPU cycles.\\n4. **L3 Cache (Level 3)**\\n    - Shared among multiple CPU cores, ranging from a few MB to tens of MB.\\n    - Acts as a buffer between L2 and RAM, reducing latency for core-to-core communication.\\n    - **Access time:** 30-60 CPU cycles.\\n5. **RAM (Random Access Memory)**\\n    - Main working memory for the system (GBs in size).\\n    - Much slower than CPU caches but holds more data.\\n    - Stores active processes and data that aren\u2019t frequently used by the CPU.\\n    - **Access time:** 100+ CPU cycles.\\n\\n## Registers: The Fastest Storage\\n### What Are Registers?\\nRegisters are ultra-fast, small storage units embedded directly inside a computer\u2019s CPU (Central Processing Unit). They are temporary holding areas for data, instructions, or memory addresses that the CPU needs to access immediately during computations. Registers are the fastest type of memory in a computer, designed to minimize delays in processing.\\n\\n### How Registers Work?\\n- **Fetching Data:** When the CPU needs to perform an operation (e.g., 5 + 3), it first copies the values 5 and 3 from RAM into two registers.\\n- **Processing:** The CPU\u2019s arithmetic logic unit (ALU) performs the addition directly using the values stored in the registers.\\n- **Storing Results:** The result (8) is placed into another register, which can either be used for further operations or written back to RAM.\\n\\n1. **Basic Structure: Flip-Flops**\\n    - **Core Component:** Registers are built using D-type flip-flops, each flip-flop just stores one bit. A 32-bit register, for example, contains 32 flip-flops.\\n    - **Function:** Each flip-flop has:\\n        - **Data Input (D):** Receives the bit to store.\\n        - **Write Enable (WE):** Controls whether the flip-flop should capture and store the input value on the next active clock edge.\\n        - **Clock Input (CLK):** Provides the timing signal that synchronizes when data is captured by the flip-flop. The flip-flop updates its value on the rising edge of the clock.\\n        - **Output (Q):** Provides the stored bit.\\n\\n2. **Data Storage and Clock Synchronization**\\n\\n    **Writing Data:**\\n    * When the CPU writes to a register:\\n        * The write enable signal for the register is activated.\\n        * Data is placed on the **input bus**.  The input bus is a set of electrical connections that carry data to the register.\\n        * In the **next rising edge of the clock**, the flip-flops capture the input values.\\n\\n    **Reading Data:**\\n    1. **Stored Data in Flip-Flops**\\n        - A register consists of multiple flip-flops, each storing a single bit. Once a value is stored in a flip-flop, it remains available at its output until changed by a new write operation. However, this stored value is not automatically placed on the CPU\u2019s internal bus\u2014something must send the data when the data is read.\\n        - Each flip-flop\'s output is connected to a tri-state buffer, which controls whether the stored bit is driven onto the bus connecting CPU and register (for reading the bit).\\n    2. **Role of the Tri-State Buffer**\\n        - Tri-State Buffer ensures conflict-free (kindof like mutex) data access, enabling the CPU to perform billions of operations per second reliably.\\n        - A tri-state buffer is a special circuit that can either:\\n            - **Tri-State Buffer Enabled:** Passes the stored data from the register to the bus connecting CPU and register.\\n            - **Tri-State Buffer Disabled:** Disconnects the register from bus connecting CPU and register.\\n        - This is necessary because multiple registers share the same internal bus, and only one should be active at a time to avoid conflicting reads and writes (more of a hardware constraint).\\n        - The enable signal is synchronized with the CPU clock to ensure stable data transfer.\\n\\n### Types of Registers (A little extra context)\\n- **General-Purpose Registers:**\\n    - Used for temporary data storage and most calculations.\\n- **Special-Purpose Registers:**\\n    - **Program Counter (PC):** Holds the memory address of the next instruction to execute.\\n    - **Instruction Register (IR):** Stores the current instruction being decoded/executed.\\n    - **Stack Pointer (SP)**: Tracks the top of the stack in memory.\\n    - **Status/Flag Register**: Stores metadata about operations (e.g., whether a result was zero or caused an overflow).\\n\\n### Why Registers Are Essential\\n- **Eliminate Bottlenecks:** Without registers, the CPU would need to read/write data directly from RAM for every operation, which is too slow.\\n- **Enable Pipelining:** Registers allow the CPU to work on multiple instructions simultaneously by holding intermediate states.\\n- **Direct Hardware Access:** Registers interface directly with the CPU\u2019s ALU and control unit, enabling rapid execution of machine-level instructions.\\n\\n## CPU Cache: L1, L2, L3 CPU caches\\nCPU Caches are small, ultra-fast memory layers between the CPU and main memory (RAM). They store frequently accessed data and instructions to reduce latency and improve performance. Modern CPUs use three levels of cache:\\n\\nThere are two types of cached instructions:\\n- **Instruction Cache:**\\n    - **What it stores:**\\n        - Instructions are the actual binary code (machine code) of the program being executed by the CPU.\\n        - Examples: `ADD`, `MOV`, `JUMP`, `LOAD`, or any operation the CPU performs.\\n    - **Purpose:**\\n        - Allows the CPU to quickly fetch the next operation to execute.\\n        - For example, when running a loop, the instruction cache holds the repeated code (`for`, `while` loops) so the CPU doesn\u2019t have to fetch it repeatedly from slower memory.\\n* **Data Cache:**\\n    * **What it stores:**\\n        * Data refers to the values the CPU is actively working with.\\n        * Examples: Variables (e.g., int x = 5), memory addresses, temporary results, or input/output values.\\n    * **Purpose:**\\n        * Provides fast access to the operands (numbers, addresses) needed by instructions.\\n        * For example, when calculating x + y, the data cache holds the values of x and y for the ADD instruction to use.\\n\\n**Why Split Them?**\\n    - **Parallel Access:**\\n        * The CPU can fetch the next instruction (from the instruction cache) while simultaneously reading/writing data (from the data cache). This avoids bottlenecks.\\n        * **Example:** While executing an ADD instruction, the CPU can already fetch the next instruction (MOV or JUMP) from the instruction cache.\\n    - **Specialization:**\\n        - Instruction caches are optimized for sequential access (program code is usually read in order).\\n        - Data caches are optimized for random access (variables can be accessed in any order).\\n\\n### L1 Cache (Level 1 Cache)\\n- **Role:**\\n    - The fastest and smallest cache, directly integrated into the CPU core.\\n    - Split into L1 Instruction Cache (stores executable code) and L1 Data Cache (stores data).\\n- **Characteristics:**\\n    * **Size:** Typically 32\u201364 KB per core (e.g., 64 KB total: 32 KB data + 32 KB instructions).\\n    * **Speed:** 1\u20134 clock cycles access time (fastest).\\n    * **Location:** Embedded within each CPU core.\\n\\n### L2 Cache (Level 2 Cache)\\n- **Role:**\\n    - Acts as a middle layer between L1 and L3.\\n    - Stores data/instructions not held in L1 but likely to be reused.\\n- **Characteristics:**\\n    - **Size:** 256 KB\u20132 MB per core (varies by CPU design).\\n    - **Speed**: 10\u201320 clock cycles access time.\\n    - **Location**: May be shared between cores or dedicated per core (e.g., AMD Zen vs. Intel Core).\\n\\n### L3 Cache (Level 3 Cache)\\n- **Role:**\\n    - The largest and slowest CPU cache, shared across all cores.\\n    - Reduces traffic to RAM by storing data shared between multiple cores.\\n- **Characteristics**:\\n    - **Size**: 4\u201364 MB \\n    - **Speed**: 20\u201350 clock cycles access time.\\n    - **Location**: On the CPU die but outside individual cores.\\n\\n### Why Three Levels?\\n- **Latency vs. Size Trade-off:** L1 prioritizes speed for critical data, L2 balances speed and size, and L3 minimizes RAM access.\\n- **Efficiency:** Reduces \\"cache misses\\" by filtering requests through layers (90% of data is often found in L1/L2).\\n- **Multicore Optimization:** L3 enables shared data (e.g., game textures, OS tasks) to stay accessible to all cores.\\n\\n**Practical Example:**\\n- When running a game:\\n    - L1: Stores code for rendering a character (e.g., position calculations).\\n    - L2: Caches textures used in the current scene.\\n    - L3: Holds shared assets like audio files or global physics data.\\n\\n### Why not replace L2 and L3 with L1?\\n- **Physical Limits:**\\n    - L1 is fast but bulky/power-hungry. Scaling it to L2/L3 sizes would make CPUs impractical (cost, heat, latency).\\n- **Hierarchy Efficiency**:\\n    - **L1**: Speed-optimized for critical data.\\n    - **L2**: Balances size/speed for common data.\\n    - **L3**: Shared, large storage to minimize RAM trips.\\n- **Cache Miss Mitigation**:\\n    - Without L2/L3, frequent RAM access (~100x slower) would cripple performance.\\n- **Power/Heat**:\\n    - Larger L1 would drain power and overheat CPUs.\\n- **Multicore Sharing**:\\n    - L3 allows cores to access shared data without duplicating it in L1/L2.\\n\\n## RAM: The Parts of the Memory Cell\\nImagine a single memory cell in your computer\u2019s RAM (the temporary memory your computer uses to do stuff). Think of it like a tiny light switch and a tiny battery working together to store a 0 or a 1 (the basic \\"yes/no\\" language computers use). Here\u2019s how it works:\\n- **Capacitor:** A tiny \u201cbattery\u201d that can hold an electric charge.\\n    * Charged (has electricity) = 1\\n    * Not charged (empty) = 0\\n- **Transistor:** A tiny \u201clight switch\u201d that controls access to the capacitor.\\n    * ON (switch closed) = Lets electricity flow.\\n    * OFF (switch open) = Blocks electricity.\\n- **Address Line:** The wire that tells the transistor to turn ON/OFF.\\n- **Data Line:** The wire that reads or writes the charge (0 or 1) to the capacitor.\\n\\n### How It Works\\n1. Writing Data (Saving a 0 or 1)\\n    - **Step 1:** The CPU (computer\u2019s brain) says, \u201cHey, I need to save a 1 at this specific memory cell!\u201d\\n    - **Step 2:** The Address Line sends electricity (like flipping the switch ON).\\n    - **Step 3:** The Data Line sends electricity to charge the capacitor (filling the tiny battery).\\n    **Result:** Capacitor is charged = 1 is stored.\\n    If the CPU wants to save a 0, the Data Line drains the capacitor instead.\\n2. Reading Data (Checking if it\u2019s 0 or 1)\\n    - **Step 1:** The CPU says, \u201cWhat\u2019s stored at this memory cell?\u201d\\n    - **Step 2:** The Address Line sends electricity (switch ON).\\n    - **Step 3:** If the capacitor is charged (storing 1), electricity flows out through the Data Line.\\n        - **Result:** The CPU detects this flow = 1.\\n    - **Step 4:** If the capacitor is empty (storing 0), no electricity flows.\\n        - **Result:** The CPU detects no flow = 0.\\n\\n### Refresh Cycle\\n- Each DRAM cell consists of a **capacitor (storing a 1 or 0 as charge)** and an **access transistor**. \\n- When a cell is **\u201ccharged\u201d (1)** or **\u201cdischarged\u201d (0)**, that state is maintained only temporarily because the charge leaks away.\\n- The **DRAM controller (or memory controller)** periodically reads each memory cell and then rewrites (recharges) it to restore the original value. This **refresh cycle** typically occurs every 64\u2013128 milliseconds for all cells.\\n- Without refreshing, the leakage would eventually cause the stored bits to flip, leading to data corruption. The periodic refresh ensures data integrity over time."},{"id":"diy_goroutines","metadata":{"permalink":"/prabhav.tech/blog/diy_goroutines","editUrl":"https://github.com/prabhavdogra/prabhav.tech/tree/master/blog/2025-03-01-goroutines/index.md","source":"@site/blog/2025-03-01-goroutines/index.md","title":"Writing your own Goroutines","description":"This all started when someone asked me how goroutines work internally and all I could respond with was:","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"Go","permalink":"/prabhav.tech/blog/tags/go","description":"Golang exploration projects"}],"readingTime":0.445,"hasTruncateMarker":false,"authors":[{"name":"Prabhav Dogra","title":"Software Engineer II @ Blinkit","url":"https://github.com/prabhavdogra","page":{"permalink":"/prabhav.tech/blog/authors/prabhavdogra"},"socials":{"github":"https://github.com/prabhavdogra","linkedin":"https://www.linkedin.com/in/prabhav-dogra/"},"imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGHJzJ1YVb_og/profile-displayphoto-shrink_800_800/B56ZS6fOAFHEAc-/0/1738295534859?e=1746057600&v=beta&t=wTqgffLNqUVaz0xEk2OUDSEvKATaSevxvWuI99mG9XY","key":"prabhavdogra"}],"frontMatter":{"slug":"diy_goroutines","title":"Writing your own Goroutines","authors":["prabhavdogra"],"tags":["go"]},"unlisted":false,"prevItem":{"title":"What does memory mean actually?","permalink":"/prabhav.tech/blog/memory_hierarchy"},"nextItem":{"title":"How Go atomic operations avoid race conditions?","permalink":"/prabhav.tech/blog/atomic_operations"}},"content":"This all started when someone asked me how goroutines work internally and all I could respond with was:\\n\\n> \\"Goroutines are lightweight threads managed by the Go runtime instead of the operating system. Go runtime automatically multiplexes\u2014mapping multiple goroutines onto a smaller number of OS threads. And that somehow makes them fast?? \ud83d\udc49\ud83d\udc48\\"\\n\\nIf anyone asked me any in-depth questions about how this multiplexing worked I was blank. So I decided to gain a deeper understanding by implementing goroutines myself. Cloned the [Go Github repo](https://github.com/golang/go)\\n\\n## To be continued..."},{"id":"atomic_operations","metadata":{"permalink":"/prabhav.tech/blog/atomic_operations","editUrl":"https://github.com/prabhavdogra/prabhav.tech/tree/master/blog/2025-02-28-atomic-operations/index.md","source":"@site/blog/2025-02-28-atomic-operations/index.md","title":"How Go atomic operations avoid race conditions?","description":"Introduction","date":"2025-02-28T00:00:00.000Z","tags":[{"inline":false,"label":"Go","permalink":"/prabhav.tech/blog/tags/go","description":"Golang exploration projects"}],"readingTime":4.29,"hasTruncateMarker":false,"authors":[{"name":"Prabhav Dogra","title":"Software Engineer II @ Blinkit","url":"https://github.com/prabhavdogra","page":{"permalink":"/prabhav.tech/blog/authors/prabhavdogra"},"socials":{"github":"https://github.com/prabhavdogra","linkedin":"https://www.linkedin.com/in/prabhav-dogra/"},"imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGHJzJ1YVb_og/profile-displayphoto-shrink_800_800/B56ZS6fOAFHEAc-/0/1738295534859?e=1746057600&v=beta&t=wTqgffLNqUVaz0xEk2OUDSEvKATaSevxvWuI99mG9XY","key":"prabhavdogra"}],"frontMatter":{"slug":"atomic_operations","title":"How Go atomic operations avoid race conditions?","authors":["prabhavdogra"],"tags":["go"]},"unlisted":false,"prevItem":{"title":"Writing your own Goroutines","permalink":"/prabhav.tech/blog/diy_goroutines"},"nextItem":{"title":"Building a Ray Tracer in C++","permalink":"/prabhav.tech/blog/ray_tracer"}},"content":"### Introduction\\nThis question popped up in my head, _\\"How Go atomic operations avoid race conditions?\\"_\\n\\nI finally gathered the courage to open the cloned [Go Github repo](https://github.com/golang/go) and scan through it.\\n\\n\\n### Go Code Structure \\n![Go code structure](go_structure.png)\\n\\n_Source: ChatGPT_\\n\\nI went inside the implementation of `CompareAndSwapInt32` and found this:\\n\\n```go title=\\"src/sync/atomic/doc.go\\"\\n// CompareAndSwapInt32 executes the compare-and-swap operation for an int32 value.\\n// Consider using the more ergonomic and less error-prone [Int32.CompareAndSwap] instead.\\n//\\n//go:noescape\\nfunc CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)\\n```\\n\\nFinding the implementation of this was not straightforward, because this method is implemented in Go Assembly:\\n```go title=\\"src/sync/atomic/asm.s\\"\\nTEXT \xb7CompareAndSwapInt32(SB),NOSPLIT,$0\\n\\tJMP\\tinternal\u2215runtime\u2215atomic\xb7Cas(SB)\\n```\\n\\n### What\'s Go Assembly?\\nSimply put, **Go Assembly** is the low-level language used to write performance-critical functions in Go.\\n**Go Assembler** (Code directory path: cmd/asm) is the tool that compiles **Go assembly (.s) files** into machine code.\\nThe Go assembler was heavily inspired by the [Plan 9 C compilers](https://9p.io/sys/doc/compiler.html).\\n\\n:::note Plan 9 C compilers\\n**Plan 9 C compilers (6c, 8c, 5c, etc.)** were architecture-specific compilers designed to generate optimized code for different CPU architectures. Unlike GCC or LLVM, which support multiple architectures within a single compiler framework, Plan 9 used separate compilers for different instruction sets. These compilers were originally developed for the Plan 9 operating system, an experimental OS designed as a potential successor to Unix-based systems.\\n\\nYou can read more about it here: [https://9p.io/sys/doc/compiler.html](https://9p.io/sys/doc/compiler.html)\\n:::\\n\\nGo drew inspiration from 9 C Compiler:\\n- Just like **Plan 9** had **separate compilers** for **different architectures** (e.g., 6c for x86-64, 8c for ARM, etc.).\\n- Go\u2019s assembler follows a similar architecture-based approach, instead of a universal assembler Go has different assemblers for x86, ARM, RISC-V, etc.\\n\\nYou can watch this, an interesting talk about [Go Assembler](https://www.youtube.com/watch?v=KINIAgRpkDA) presented by Rob Pike himself.\\n\\n[Go Assembler Documentation](https://go.dev/doc/asm)\\n\\nGo Assembler streamlined a lot of things:\\n- **Portability:** It abstracts CPU architecture details better.\\n- **Simpler syntax:** No need for % prefixes, brackets, or complex addressing.\\n- **Unified across architectures:** ARM, AMD64, RISC-V, etc., use the same structure.\\n- **Designed for the Go runtime:** Helps implement Go features like garbage collection, goroutines, and stack growth efficiently.\\n\\n\\nGo Assembler has 4 architecture-specific implementations of `atomic.CompareAndSwapInt32()`:\\n- **amd64.s**: For AMD64 (x86-64) architecture (Intel, AMD CPUs).\\n- **arm64.s:** For ARM64 (AArch64) processors (used in Apple M1/M2, mobile devices, servers).\\n- **ppc64le.s:** For PowerPC 64-bit, Little Endian (used in IBM systems).\\n- **s390x.s:** For IBM Z-series mainframes (used in enterprise computing).\\n\\nGo runs on multiple architectures, and low-level atomic operations must be natively implemented for each to ensure compatibility.\\n\\nAdded the implementations for one architecture (other 3 are similar) in Go Assembly:\\n\\n\\n```go title=\\"src/internal/runtime/atomic/atomic_amd64.s\\"\\n// bool Cas(int32 *val, int32 old, int32 new)\\n// Atomically:\\n//\\tif(*val == old){\\n//\\t\\t*val = new;\\n//\\t\\treturn 1;\\n//\\t} else\\n//\\t\\treturn 0;\\n//  }\\nTEXT \xb7Cas(SB),NOSPLIT,$0-17\\n\\tMOVQ\\tptr+0(FP), BX\\n\\tMOVL\\told+8(FP), AX\\n\\tMOVL\\tnew+12(FP), CX\\n\\tLOCK\\n\\tCMPXCHGL\\tCX, 0(BX)\\n\\tSETEQ\\tret+16(FP)\\n\\tRET\\n```\\nExplaining this line by line how this maintains atomicity.\\n\\n```go\\nTEXT \xb7Cas(SB),NOSPLIT,$0-17\\n```\\n- `TEXT \xb7Cas(SB)`: Declares the function Cas(CompareAndSwap) in Go assembly.\\n- `NOSPLIT`: Instructs the runtime not to perform stack splitting, ensuring that the function runs without interruption. It tells the Go runtime not to perform stack splitting for that function. \\n- `$0-17:` Specifies the stack frame size for the function (0 bytes for local variables and 17 bytes for arguments/return values).\\n\\n\\n```go\\nMOVQ ptr+0(FP), BX:\\n```\\n- Moves the pointer `ptr` (the address of val) from the function\'s frame pointer (FP) into the `BX` register.\\n```go\\nMOVL old+8(FP), AX:\\n```\\n- Moves the old value from the frame pointer into the `AX` register.\\n```go\\nMOVL new+12(FP), CX:\\n```\\n- Moves the new value from the frame pointer into the `CX` register.\\n```go\\nLOCK:\\n```\\n- This is a crucial instruction. It prefixes the next instruction (CMPXCHGL) with a lock, ensuring that the memory operation is atomic. This lock ensures that no other process or thread can modify the memory location while the compare and exchange instruction is running.\\n```go\\nCMPXCHGL CX, 0(BX):\\n```\\n- This is the Compare and Exchange instruction. It performs the following:\\n\\t- Compares the value in AX (the old value) with the value at the memory location pointed to by BX (the val value).\\n\\t- If the values are equal, it replaces the value at 0(BX) with the value in CX (the new value).\\n\\t- The original value at 0(BX) is loaded into the AX register.\\n```go\\nSETEQ ret+16(FP):\\n```\\n- SETEQ sets the byte at the destination to 1 if the zero flag is set, and to 0 otherwise. In this case, it sets the return value to 1 if the comparison was equal (meaning the swap was successful), and to 0 otherwise.\\n```go\\nRET:\\n```\\n- Returns from the function\\n\\n\\n### Conclusion\\n\\nAt the register level, atomicity is achieved because:\\n- The LOCK prefix serializes access across CPU cores.\\n- CMPXCHGL ensures all three steps (compare, swap, write-back) happen as one unit.\\n- The CPU guarantees atomicity, eliminating race conditions without software locks.\\n\\nFeel free to be curious and figure out the answers to your questions on your own."},{"id":"ray_tracer","metadata":{"permalink":"/prabhav.tech/blog/ray_tracer","editUrl":"https://github.com/prabhavdogra/prabhav.tech/tree/master/blog/2025-02-27-ray-tracer/index.md","source":"@site/blog/2025-02-27-ray-tracer/index.md","title":"Building a Ray Tracer in C++","description":"This blog is just a quick summary of the book Ray Tracing in One Weekend","date":"2025-02-27T00:00:00.000Z","tags":[{"inline":false,"label":"C++","permalink":"/prabhav.tech/blog/tags/c++","description":"C++ Projects"}],"readingTime":9.165,"hasTruncateMarker":false,"authors":[{"name":"Prabhav Dogra","title":"Software Engineer II @ Blinkit","url":"https://github.com/prabhavdogra","page":{"permalink":"/prabhav.tech/blog/authors/prabhavdogra"},"socials":{"github":"https://github.com/prabhavdogra","linkedin":"https://www.linkedin.com/in/prabhav-dogra/"},"imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGHJzJ1YVb_og/profile-displayphoto-shrink_800_800/B56ZS6fOAFHEAc-/0/1738295534859?e=1746057600&v=beta&t=wTqgffLNqUVaz0xEk2OUDSEvKATaSevxvWuI99mG9XY","key":"prabhavdogra"}],"frontMatter":{"slug":"ray_tracer","title":"Building a Ray Tracer in C++","authors":["prabhavdogra"],"tags":["c++"]},"unlisted":false,"prevItem":{"title":"How Go atomic operations avoid race conditions?","permalink":"/prabhav.tech/blog/atomic_operations"}},"content":"This blog is just a quick summary of the book [Ray Tracing in One Weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.htmlRay)\\n\\nGithub Source Code: [dograprabhav/ray_tracer](https://github.com/dograprabhav/ray_tracer)\\n\\n## What\'re we gonna make?\\n\\n![Final](image-2.png)\\n\\n## Milestone 1:\\nBefore starting anything, in this milestone we just create a sample image. To do that we use one of the simplest formats **P3**. **P3** is a plain text format for **Portable Pixmap (PPM)** image files. It is one of the simplest image formats, where pixel data is represented in plain text.\\n\\n### P3 Image Format\\nIn P3, each pixel is defined by three integers corresponding to the red, green, and blue color channels. The first line of the output is \\"P3\\", identifying the file format. The second line contains the width and height of the image. The third line specifies the maximum color value (typically 255, representing the maximum intensity for each color channel).\\nEach subsequent line contains three integers (r, g, b) for each pixel\'s color in the image.\\n\\n```text title=\\"Sample P3 image of width 2 pixels and height 3 pixels\\"\\nP3           // Defining format\\n2 3          // Width and Height\\n255          // Maximum color value\\n2 5 15       // (r, g, b) color intensity triplets\\n0 255 255    // (r, g, b) color intensity triplets\\n255 0 255    // (r, g, b) color intensity triplets\\n255 255 0    // (r, g, b) color intensity triplets\\n0 0 0        // (r, g, b) color intensity triplets\\n255 255 255  // (r, g, b) color intensity triplets\\n```\\nThis image looks like: &nbsp; &nbsp; &nbsp; ![P3](image-3.png)\\n\\n### Milestone 1 Results\\nWe write a simple loop to render this:\\n\\n![P3](image-4.png)\\n\\n\\n## Milestone 2:\\nIn this milestone we setup a basic ray tracing setup. \\n- We setup a sphere in the scene.\\n- We setup light rays that detects object in the scene.\\n- On the basis of intersection of light rays and objects in the scene it detects what each pixel in the image should look like.\\n- Set up vector ray header files\\n\\n### How it works?\\n![How it works?](image-5.png)\\nThis image represents the basic concept of a ray-tracing camera model used in computer graphics and rendering. Let\u2019s break it down step by step:\\n\\n* **Camera and Viewport Setup**: The camera center is the origin of the coordinate system. A viewport (camera screen/a plane) is placed in front of the camera at a certain focal length. The viewport is divided into a grid of pixels (X \xd7 Y), where each cell represents a pixel in the final rendered image.\\n\\n* **Ray Tracing Process:** For each pixel in the viewport grid\\n    - A ray of light is cast from the camera center through the center of the pixel.\\n    - The ray travels in the scene and intersects with objects (like the blue sphere in the diagram).\\n    - If a ray hits an object, the rendering algorithm calculates the color of that pixel based on:\\n        - The material properties (color, reflectivity, transparency).\\n        - Lighting conditions (shadows, reflections, refractions).\\n        - Camera perspective.\\n    - The computed color is assigned to the corresponding pixel in the final image.\\n\\n### Sphere-Ray Intersection\\nThis section explains the mathematical derivation for determining the intersection points between a ray and a sphere.\\n\\n**Sphere Equations**\\n\\n* **Sphere centered at (0, 0, 0):**\\n    * `x\xb2 + y\xb2 + z\xb2 = r\xb2`\\n* **Sphere centered at (Cx, Cy, Cz):**\\n    * `(Cx - x)\xb2 + (Cy - y)\xb2 + (Cz - z)\xb2 = r\xb2`\\n\\n**Vector and Distance**\\n\\n* **Vector from point A(x1, y1, z1) to B(x2, y2, z2):**\\n    * `(B - A) = (x2 - x1, y2 - y1, z2 - z1)`\\n* **Distance between points A and B:**\\n    * `d = \u221a[(x2 - x1)\xb2 + (y2 - y1)\xb2 + (z2 - z1)\xb2]`\\n* **Vector from point P(x, y, z) to center C(Cx, Cy, Cz):**\\n    * `(C - P) = (Cx - x, Cy - y, Cz - z)`\\n\\n**Point on Sphere Condition**\\n\\nFor point P to lie on the sphere, it must be \'r\' (radius) distance from the center:\\n\\n* `(C - P) \u22c5 (C - P) = (Cx - x)\xb2 + (Cy - y)\xb2 + (Cz - z)\xb2`\\n* `(Cx - x)\xb2 + (Cy - y)\xb2 + (Cz - z)\xb2 = r\xb2` (Distance from center)\\n* Therefore: `(C - P) \u22c5 (C - P) = r\xb2`\\n\\n**Ray Equation**\\n\\n* **General ray equation:** `RAY(t) = M * t + N`\\n    * `M` is the ray\'s direction vector.\\n    * `N` is the ray\'s origin point.\\n\\n**Ray-Sphere Intersection**\\n\\nA ray hits the sphere when it\'s \'r\' distance from the center:\\n\\n* `(C - RAY(t)) \u22c5 (C - RAY(t)) = r\xb2`\\n* `(C - (M * t + N)) \u22c5 (C - (M * t + N)) = r\xb2`\\n* Expanding the equation:\\n    * `t\xb2 * M \u22c5 M - 2 * t * M \u22c5 (C - N) + (C - N) \u22c5 (C - N) - r\xb2 = 0`\\n\\n**Quadratic Formula**\\n\\nUsing the quadratic formula (`roots = -b \xb1 \u221a(b\xb2 - 4ac) / 2a`), we get:\\n\\n* `a = M \u22c5 M`\\n* `b = -2 * M \u22c5 (C - N)`\\n* `c = (C - N) \u22c5 (C - N) - r\xb2`\\n\\nBy solving this quadratic equation for \'t\', we can find the intersection points (if any) between the ray and the sphere.\\n\\n### Milestone 2 Results\\n![Milestone 2 results](image-6.png)\\n\\n## Milestone 3:\\n- Scene Abstraction:\\n    - Introduced a Scene structure to manage all objects, lights, and properties in the environment.\\n    - Simplifies the rendering process by treating the scene as a collection of objects instead of handling each separately.\\n\\n**Result:** The code is cleaner, modular, and easier to extend in the future (e.g., adding reflections, refractions, and different shapes).\\n\\n### Milestone 3 Results\\n![Milestone 3 results](image-7.png)\\n\\n## Milestone 4:\\n**Rendering Improvements**  \\n- **Added Anti-Aliasing**  \\n  - Reduced jagged edges in the final image by averaging multiple rays per pixel.\\n- **Added Camera Class**  \\n  - Abstracted camera logic for better scene control.\\n- **Started Considering Reflected Rays**  \\n  - Introduced initial logic for reflection to create mirror-like surfaces.  \\n  - Prepares the system for handling realistic light behavior.\\n\\n**Material System Enhancements**  \\n- **Added Material Class for Diffuse Material**  \\n  - Defined a reusable `Material` class to manage object properties.  \\n  - Simplified code structure by encapsulating material behavior.  \\n- **Added Material Class with Diffuse Material**  \\n  - Implemented Lambertian reflection for diffuse surfaces.  \\n  - Ensures objects interact naturally with light sources.  \\n- **Added True Lambertian Reflection**  \\n  - Improved light scattering on rough surfaces.  \\n  - Used a more accurate random sampling technique for diffuse reflections.\\n- **Gamma Correction**\\n  - Gamma Correction for More Realistic Colors\\n\\n\\n### Anti-aliasing\\n- Reduced jagged edges in the final image by averaging multiple rays per pixel.\\n![Anti-Aliasing](image-8.png)\\n\\n### A Simple Diffuse Material\\n\\nA diffuse surface is a surface that scatters light in many directions instead of reflecting it in a single, well-defined direction (like a mirror). This happens because the surface is rough at a microscopic level.\\n\\n* Some observations:\\n  * A light ray that bounces of a diffuse surface has equal probability of bouncing in all directions\\n  * They might also be absorbed rather than reflected. The darker the surface, the more likely the ray is absorbed (that\u2019s why it\'s dark!). \\n\\n\\n- How we will do it:\\n  - Generate a random vector inside the unit sphere\\n  - Normalize this vector to extend it to the sphere surface\\n  - Invert the normalized vector if it falls onto the wrong hemisphere\\n\\n![Generating randomised reflected rays](image.png)\\n\\n### True Lambertian Reflection\\n\\nA more accurate representation of real diffuse objects is the Lambertian distribution. This distribution scatters reflected rays in a manner that is proportional to cos(\ud835\udf19), where \ud835\udf19 is the angle between the reflected ray and the surface normal.\\n\\nThis means that a reflected ray is most likely to scatter in a direction near the surface normal, and less likely to scatter in directions away from the normal.\\nWe do this by\\n\\n```c title=\\"src/v4/camera.h\\"\\n    // rec.normal is the normal to the hemisphere\\n    vec3 direction = rec.normal + random_unit_vector();\\n```\\n### Gamma Correction\\n* Gamma Correction for More Realistic Colors\\n  * Raw pixel values in the renderer are stored in linear color space.\\n  * Most displays, however, interpret color values in a non-linear way, requiring gamma correction.\\n  * Gamma correction is applied using the equation:\\n  ```\\n  corrected color = raw color ^ (\xb9\u2044\u1d67)\\n  ```\\n  where **\u03b3** (gamma) is typically **2.2**.  \\n\\n## Milestone 5:\\n**Metal**\\n- Add a new class to **Material class** for **Metal object**\\n- Modelled light scatter and **reflectance**, enabling realistic surfaces.\\n- Added **mirrored light reflection** for metallic objects.\\n- Implemented **fuzzy reflection**, simulating rough metallic finishes.\\n\\n**Dielectrics**\\n* Explored refraction and how light bends through transparent materials.\\n* Used **Snell\u2019s Law** to determine how rays change direction at surfaces.\\n* Introduced **total internal reflection**, where light stays within the medium.\\n* Implemented the **Schlick Approximation** for **realistic reflection intensity**.\\n\\n**Positionable Camera**\\n- Defines camera viewing geometry for perspective accuracy.\\n- Introduces controls for positioning and orienting the camera, improving scene setup.\\n\\n### Metallic Surfaces and Reflective Rays\\nIntroduces metallic materials by modifying how rays bounce off surfaces.\\nReflection is modeled using the equation:\\n```\\n\ud835\udc45 = \ud835\udc49 \u2212 2 (\ud835\udc49 \u22c5 \ud835\udc41) \ud835\udc41 \\n```\\n\\n![Mirrored reflection](image-9.png)\\n* Where,\\n    * **\ud835\udc45** is the reflected ray,\\n    * **\ud835\udc49** is the incoming ray, and \\n    * **\ud835\udc41** is the surface normal.\\nThe reflected ray is traced to determine the color contribution from the metal.\\n\\n- Fuzziness in Reflection:\\n  - To simulate rough metal, a fuzziness parameter is introduced.\\n  - Instead of perfect reflection, a small random offset is added to the reflected ray direction.\\n  - The amount of fuzziness controls how polished or rough the surface appears.\\n\\n### Dielectrics\\n\\nClear materials such as water, glass, and diamond are dielectrics. When a light ray hits them, it splits into a reflected ray and a refracted (transmitted) ray. We\u2019ll handle that by randomly choosing between reflection and refraction, only generating one scattered ray per interaction.\\n\\n### Snell\'s Law\\nWhere \ud835\udf03 and \ud835\udf03\u2032 are the angles from the normal, and \ud835\udf02 and \ud835\udf02\u2032 are the refractive indices. The geometry is:\\n```\\n\u21d2 \ud835\udf02 \u22c5 sin\ud835\udf03 = \ud835\udf02\u2032 \u22c5 sin\ud835\udf03\u2032\\n```\\nwhere:  \\n- **\ud835\udf02** \u2014 Refractive index of the first medium  \\n- **\ud835\udf02\u2032** \u2014 Refractive index of the second medium  \\n- **\ud835\udf03** \u2014 Incident angle (angle between the incoming ray and the normal)  \\n- **\ud835\udf03\u2032** \u2014 Refracted angle (angle between the refracted ray and the normal)  \\n\\n![Snell\'s Law](image-1.png)\\n\\n### Total Internal Reflection\\n\\nTotal Internal Reflection (TIR) occurs when light traveling from a **denser medium** to a **less dense medium** is **completely reflected** rather than refracted. This happens when the **angle of incidence** exceeds the **critical angle**, given by:\\n\\n```\\ntheta = sin\u207b\xb9(n\u2082 / n\u2081)\\n```\\n\\n![TIR](image-11.png)\\n\\n### Schlick\'s Approximation\\n\\nSchlick\'s Approximation provides an efficient way to estimate **reflectance** at the interface of two materials based on the **angle of incidence**. \\n\\n![schlicks](image-12.png)\\n\\n```\\nR\u2080 = ((n\u2081 - n\u2082) / (n\u2081 + n\u2082))\xb2\\n```\\nThis approximation avoids expensive computations while providing visually accurate reflections.\\n\\n### Final Render\\n\\n![Final](image-2.png)"}]}}')}}]);